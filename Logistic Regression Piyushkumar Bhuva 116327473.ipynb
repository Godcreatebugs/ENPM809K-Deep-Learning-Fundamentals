{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression as a Single Neuran of a Neural Network\n",
    "\n",
    "Welcome to your first programming assignment! You will build a logistic regression classifier to recognize  cats. This assignment will step you through how to do this with a Neural Network mindset where logistic regression represents a single nueron. \n",
    "\n",
    "**Instructions:**\n",
    "- Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.\n",
    "\n",
    "**You will learn to:**\n",
    "- Build the general architecture of a learning algorithm, including:\n",
    "    - Initializing parameters\n",
    "    - Calculating the cost function and its gradient\n",
    "    - Using an optimization algorithm (gradient descent) \n",
    "- Gather all three functions above into a main model function, in the right order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 -  Loading Packages ##\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: #importing libraries and modules\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from load_dataset import*\n",
    "%matplotlib inline  #importing libraries and modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Utility functions to convert images into datasets ##\n",
    "The following functions are used to convert the cats and dogs images in the dataset folder into the numpy array format with labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_data(image, WIDTH, HEIGHT):\n",
    "    image_resized = Image.open(image).resize((WIDTH, HEIGHT))\n",
    "    image_array = np.array(image_resized).T\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_train_test_data(num_pix,test_size=0.2):\n",
    "    cat_files = glob.glob(\"datasets/cat*\")\n",
    "    dog_files = glob.glob(\"datasets/dog*\")\n",
    "\n",
    "    # Restrict cat and dog files here for testing\n",
    "    cat_list = [convert_image_to_data(i, num_pix, num_pix) for i in cat_files]\n",
    "    dog_list = [convert_image_to_data(i, num_pix, num_pix) for i in dog_files]\n",
    "\n",
    "    y_cat = np.zeros(len(cat_list))\n",
    "    y_dog = np.ones(len(dog_list))\n",
    "\n",
    "    X = np.concatenate([cat_list, dog_list])\n",
    "    X = np.concatenate([cat_list, dog_list])\n",
    "    y = np.concatenate([y_cat, y_dog])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Load data set ###\n",
    "Use the utility functions provided above to load the train_set_x,train_set_y, test_set_x, test_set_y.\n",
    "Set the `num_pix` to 64 and keep the `test_size` as the default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "num_pix = 64  #  image is 64 pixels \n",
    "\n",
    "datasets = load_dataset(\"car_dataset.hdf5\")\n",
    "\n",
    "#The below data will be used to train neural network. We will specifically use indexing to get info in array form.\n",
    "#first training dat awill be loaded and then all sequenses will be implemented\n",
    "x = np.array(datasets[\"train\"][\"X\"])#used indexing\n",
    "y = np.array(datasets[\"train\"][\"Y\"])\n",
    "\n",
    "#This is test data which used to define accuracy.\n",
    "x1 = np.array(datasets[\"dev\"][\"X\"])\n",
    "y1 = np.array(datasets[\"dev\"][\"Y\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 - Print the shapes ###\n",
    "Print the shape of the following variables\n",
    "- Number of training examples: m_train\n",
    "- Number of testing examples: m_test\n",
    "- Height/Width of each image: num_px\n",
    "- train_set_x shape\n",
    "- train_set_y shape\n",
    "- test_set_x shape\n",
    "- test_set_y shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples:  517\n",
      "Number of testing examples:  172\n",
      "Height/Width of each image: 64\n",
      "(517, 64, 64, 3)\n",
      "(517,)\n",
      "(172, 64, 64, 3)\n",
      "(172,)\n"
     ]
    }
   ],
   "source": [
    "#printing shapes of different matrices\n",
    "num_pix = 64\n",
    "print(\"Number of training examples: \", x.shape[0])#no of images to train data where x is train set\n",
    "print(\"Number of testing examples: \", x1.shape[0])#no of test images to define model accuracy \n",
    "print(\"Height/Width of each image:\", num_pix) #pixel height and width\n",
    "print (x.shape)\n",
    "print (y.shape)\n",
    "print (x1.shape)\n",
    "print (y1.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 - Fixing ill-shape vectors ####\n",
    "It is possible that the train_set_y and test_set_y have an ill-shape. Fix these shapes so the train_set_y and test_set_y are represented as a matrix with size (1, number of examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_y_f shape: (1, 517)\n",
      "test_set_y_f shape: (1, 172)\n"
     ]
    }
   ],
   "source": [
    "# Reshaping the vectors\n",
    "y_flat = y.reshape(y.shape[0], -1).T\n",
    "y1_flat = y1.reshape(y1.shape[0], -1).T\n",
    "\n",
    "yp_train_set = y_flat\n",
    "yc_test_set = y1_flat\n",
    "\n",
    "# This is a cat/dog dataset\n",
    "\n",
    "# your code here <end>\n",
    "\n",
    "print (\"train_set_y_f shape: \" + str(y_flat.shape)) #concatanating strings as it represent y in proper shape\n",
    "print (\"test_set_y_f shape: \" + str(y1_flat.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Flatten the data\n",
    "Reshape the training and test data sets so that each image is flattened into single vectors of shape (num_px  ∗ num_px  ∗ 3, 1). Check the shapes for train_set_x_flatten and test_set_x_flatten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x shape: (12288, 517)\n",
      "test_set_x shape: (12288, 172)\n"
     ]
    }
   ],
   "source": [
    "#flatten the data matrix using choosing a 0th column\n",
    "x_flat = x.reshape(x.shape[0], -1).T\n",
    "\n",
    "x1_flat = x1.reshape(x1.shape[0], -1).T\n",
    "\n",
    "\n",
    "print (\"train_set_x shape: \" + str(x_flat.shape))\n",
    "\n",
    "print (\"test_set_x shape: \" + str(x1_flat.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Standardize the data\n",
    "Divide every row of the dataset by 255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing flattened data by 255\n",
    "yp_train_set = x_flat/255 \n",
    "yp_test_set = x1_flat/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Implementing the Helper Functions ## \n",
    "\n",
    "### 3.1 - Sigmoid function\n",
    "Implement `sigmoid()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a sigmoid function mathematically where z is variable\n",
    "def sigmoid(z):\n",
    "    \n",
    "\n",
    "    s = 1/(1+ np.exp(-z)) #array and exponential combined\n",
    "    \n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Implement initialize_with_zeros\n",
    "Write a function that initializes initialize w as a vector of zeros and set `b` to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#as indicated we will initialise w as vector of zero\n",
    "def initialize_with_zeros(dim):#black represent the 0 value\n",
    "    \n",
    "    \n",
    "    \n",
    "    w = np.zeros(shape=(dim,1))\n",
    "    b = 0.0\n",
    "   \n",
    "\n",
    "    assert(w.shape == (dim\n",
    "                       , 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Forward propagation\n",
    "\n",
    "Implement forward propagation to calculate $A$ and cost.\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a function for forward propogation\n",
    "def forward_propogation(w,b,X,Y) :#used previous data as an argument\n",
    "    \n",
    "    m = X.shape[1]  \n",
    "   \n",
    "    Z= np.dot(w.T,X)+b\n",
    "    A = sigmoid(Z)   #already defined this function  \n",
    "    cost = (-1/m) * np.sum(Y* np.log(A)+ (1- Y) * np.log(1-A))  #cost will smooth the curve\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost,A,m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Backward propagation\n",
    "\n",
    "Implement backward propagation to compute gradients $dw$ and $db$\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ dw = \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$\n",
    "$$ db = \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making function for backward propogation\n",
    "def backward_propogation(w,b,X,Y) : #same as forward_propogation\n",
    "    m = X.shape[1]  \n",
    "    Z= np.dot(w.T,X)+b\n",
    "    A = sigmoid(Z) \n",
    "    \n",
    "    dz = A - Y #this are small changes in directions so indicated as differentiation\n",
    "    dw = (1/m) * np.dot(X,dz.T)\n",
    "    db = (1/m) * np.sum(dz)\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    \n",
    "    return dw,db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 - Optimization\n",
    "- You have initialized your parameters.\n",
    "- You are also able to compute a cost function and its gradient.\n",
    "- Now, you want to update the parameters using gradient descent.\n",
    "\n",
    "Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining an optimizing function based on which we can optimize the cost\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost= False) :\n",
    "    \n",
    "    costs = [] #costs defined as a list in which values will be stored\n",
    "    for i in range(num_iterations) :\n",
    "\n",
    "        cost,A,m = forward_propogation(w,b,X,Y)\n",
    "        dw,db = backward_propogation(w,b,X,Y)\n",
    "\n",
    "        w = w-learning_rate*dw #learning rate \n",
    "        b = b-learning_rate*db\n",
    "\n",
    "        if i % 100 == 0:  \n",
    "                costs.append(cost)\n",
    "\n",
    "        if print_cost and i % 100 == 0:\n",
    "                print (\"Cost after iteration %i: %f\" %(i,cost))\n",
    "\n",
    "    return dw,db,w,b,costs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 - Prediction\n",
    "\n",
    "Implement the `predict()` function. There is two steps to computing predictions:\n",
    "\n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector `Y_prediction`. If you wish, you can use an `if`/`else` statement in a `for` loop (though there is also a way to vectorize this). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining prediction\n",
    "def predict(w, b, X_test):\n",
    "    \n",
    "    \n",
    "    m = X_test.shape[1]#using slicing to grab data sets\n",
    "    \n",
    "    w = w.reshape(X_test.shape[0], 1)\n",
    "    \n",
    "    \n",
    "    Z= np.dot(w.T,X_test) + b\n",
    "    A = sigmoid(Z)\n",
    "   \n",
    "    \n",
    "    Y_prediction = np.around(A)\n",
    "    \n",
    "      \n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Regression Model ##\n",
    "Implement the model function. Use the following notation:\n",
    "    - Y_prediction for your predictions on the test set\n",
    "    - Y_prediction_train for your predictions on the train set\n",
    "    - w, costs, grads for the outputs of optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining regressinal model with arguments defined earlier\n",
    "def reg_model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "       \n",
    "    # setting parameters to zeros to initialise \n",
    "    n= X_train.shape[0]\n",
    "    w, b = initialize_with_zeros(dim=n)#initialising with zeros\n",
    "    \n",
    "\n",
    "    # arguments decide the optimiztions and they hAave to go in respective order\n",
    "    dw,db,w,b,costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Examples of prediction data sets like train and test set examples\n",
    "    Y_prediction_test = predict(w,b,X_test)\n",
    "    Y_prediction_train = predict(w,b,X_train)\n",
    "\n",
    "    \n",
    "    # error printing with acuuracy with just doing minus from 100 \n",
    "    print(\"accuracy_train: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"accuracy_test: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the regression model function to train your model.\n",
    "### 5.1 - Setting parameters (part 1)\n",
    "Set the `num_iterations` to 5000 and `learning_rate` to 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 517)\n",
      "(1, 517)\n",
      "(12288, 173)\n",
      "(1, 173)\n",
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.548509\n",
      "Cost after iteration 200: 0.496297\n",
      "Cost after iteration 300: 0.459536\n",
      "Cost after iteration 400: 0.432244\n",
      "Cost after iteration 500: 0.411066\n",
      "Cost after iteration 600: 0.394034\n",
      "Cost after iteration 700: 0.379940\n",
      "Cost after iteration 800: 0.368004\n",
      "Cost after iteration 900: 0.357705\n",
      "Cost after iteration 1000: 0.348679\n",
      "Cost after iteration 1100: 0.340667\n",
      "Cost after iteration 1200: 0.333477\n",
      "Cost after iteration 1300: 0.326965\n",
      "Cost after iteration 1400: 0.321019\n",
      "Cost after iteration 1500: 0.315553\n",
      "Cost after iteration 1600: 0.310498\n",
      "Cost after iteration 1700: 0.305796\n",
      "Cost after iteration 1800: 0.301404\n",
      "Cost after iteration 1900: 0.297283\n",
      "Cost after iteration 2000: 0.293401\n",
      "Cost after iteration 2100: 0.289732\n",
      "Cost after iteration 2200: 0.286254\n",
      "Cost after iteration 2300: 0.282948\n",
      "Cost after iteration 2400: 0.279796\n",
      "Cost after iteration 2500: 0.276785\n",
      "Cost after iteration 2600: 0.273902\n",
      "Cost after iteration 2700: 0.271136\n",
      "Cost after iteration 2800: 0.268477\n",
      "Cost after iteration 2900: 0.265917\n",
      "Cost after iteration 3000: 0.263449\n",
      "Cost after iteration 3100: 0.261066\n",
      "Cost after iteration 3200: 0.258761\n",
      "Cost after iteration 3300: 0.256529\n",
      "Cost after iteration 3400: 0.254366\n",
      "Cost after iteration 3500: 0.252267\n",
      "Cost after iteration 3600: 0.250228\n",
      "Cost after iteration 3700: 0.248245\n",
      "Cost after iteration 3800: 0.246316\n",
      "Cost after iteration 3900: 0.244436\n",
      "Cost after iteration 4000: 0.242604\n",
      "Cost after iteration 4100: 0.240816\n",
      "Cost after iteration 4200: 0.239071\n",
      "Cost after iteration 4300: 0.237366\n",
      "Cost after iteration 4400: 0.235700\n",
      "Cost after iteration 4500: 0.234070\n",
      "Cost after iteration 4600: 0.232474\n",
      "Cost after iteration 4700: 0.230912\n",
      "Cost after iteration 4800: 0.229381\n",
      "Cost after iteration 4900: 0.227881\n",
      "accuracy_train: 92.06963249516441 %\n",
      "accuracy_test: 25.100306018361096 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#taken 'dev' dataset and doing test\n",
    "\n",
    "\n",
    "# Taken 'test' as dataset and performing actions\n",
    "x = np.array(datasets[\"train\"][\"X\"])\n",
    "x_flat = x.reshape(x.shape[0], -1).T\n",
    "xp1_train_set = x_flat/255\n",
    "print (xp1_train_set.shape)# printing shape\n",
    "\n",
    "y = np.array(datasets[\"train\"][\"Y\"])\n",
    "y_flat = y.reshape(y.shape[0], -1).T\n",
    "yp1_train_set = y_flat\n",
    "print (yp1_train_set.shape)\n",
    "\n",
    "\n",
    "x1 = np.array(datasets[\"test\"][\"X\"])\n",
    "x1_flat = x1.reshape(x1.shape[0], -1).T\n",
    "xp1_test_set = x1_flat/255\n",
    "print (xp1_test_set.shape)\n",
    "\n",
    "y1 = np.array(datasets[\"test\"][\"Y\"])\n",
    "y1_flat = y1.reshape(y1.shape[0], -1).T\n",
    "yp1_test_set = y1_flat/255\n",
    "print (yp1_test_set.shape)\n",
    "\n",
    "costs= reg_model(xp1_train_set, yp1_train_set, xp1_test_set, yp1_test_set,  num_iterations = 5000, learning_rate = 0.0005, print_cost = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:40%\"> \n",
    "    \n",
    "    <tr>\n",
    "        <td> **Train Accuracy**  </td> \n",
    "        <td> 91.25 % </td>\n",
    "    </tr>\n",
    "\n",
    "    <tr>\n",
    "        <td>**Test Accuracy** </td> \n",
    "        <td> 60.0 % </td>\n",
    "    </tr>\n",
    "</table> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the cost function and the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.548509\n",
      "Cost after iteration 200: 0.496297\n",
      "Cost after iteration 300: 0.459536\n",
      "Cost after iteration 400: 0.432244\n",
      "Cost after iteration 500: 0.411066\n",
      "Cost after iteration 600: 0.394034\n",
      "Cost after iteration 700: 0.379940\n",
      "Cost after iteration 800: 0.368004\n",
      "Cost after iteration 900: 0.357705\n",
      "Cost after iteration 1000: 0.348679\n",
      "Cost after iteration 1100: 0.340667\n",
      "Cost after iteration 1200: 0.333477\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# plot for learning rate = 0.0005 and num_iterations as 5000 ['test set']\n",
    "d = reg_model(xp1_train_set, yp1_train_set, xp1_test_set, yp1_test_set,  num_iterations = 5000, learning_rate = 0.0005, print_cost = True)\n",
    "costs =np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.title(\"Plot 1\")\n",
    "plt.xlabel('Iterations (per hundred)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 - Setting parameters (part 2)\n",
    "Set the `num_iterations` to 2000 and `learning_rate` to 0.005 and run the model again. Plot the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=reg_model(xp1_train_set, yp1_train_set, xp1_test_set, yp1_test_set,  num_iterations = 2000, learning_rate = 0.005, print_cost = True)\n",
    "costs =np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.title(\"Plot 2\")\n",
    "plt.xlabel('Iterations (per hundred)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 - Analysis\n",
    "Compare the cost function plots of part_1 and part_2. Write your observation below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Your observation here\n",
    "\n",
    "We can derive a difference between plot1 nad plot2. plot1 has much higher learning rate than plot2. As compared to iteration, number of iterations in plot1 is around 5000 compared to number of iterations is 2000 in plot2. \n",
    "1. The obsevation shows that learning rate increases when we decrease iterations . \n",
    "2. If we decrease learning rate then cost function will approach global minimum which is better.\n",
    "3. Accuracy of model 2 is greater than Model1 becuase of iterations.\n",
    "4. In increasing complexity for the same problem then only higher no of iteration and lower value of learning rate provides reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XaIWT",
   "launcher_item_id": "zAgPl"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
